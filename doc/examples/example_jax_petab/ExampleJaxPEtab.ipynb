{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d2bc5c",
   "metadata": {},
   "source": [
    "# Simulating AMICI models using JAX\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide demonstrates how to use AMICI to export models in a format compatible with the [JAX](https://jax.readthedocs.io/en/latest/) ecosystem, enabling simulations with the [diffrax](https://docs.kidger.site/diffrax/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fe897",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "To begin, we will import a model using [PEtab](https://petab.readthedocs.io). For this demonstration, we will utilize the [Benchmark Collection](https://github.com/Benchmarking-Initiative/Benchmark-Models-PEtab), which provides a diverse set of models. For more information on importing PEtab models, refer to the corresponding [PEtab notebook](https://amici.readthedocs.io/en/latest/petab.html).\n",
    "\n",
    "In this tutorial, we will import the Böhm model from the Benchmark Collection. Using [amici.petab_import](https://amici.readthedocs.io/en/latest/generated/amici.petab_import.html#amici.petab_import.import_petab_problem), we will load the PEtab problem. To create a  [JAXProblem](https://amici.readthedocs.io/en/latest/generated/amici.jax.html#amici.jax.JAXProblem) instead of a standard AMICI model, we set the `jax` parameter to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ade24ee5aca07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import petab.v1 as petab\n",
    "from amici.importers.petab import *\n",
    "from petab.v2 import petab1to2\n",
    "\n",
    "# Define the model name and YAML file location\n",
    "model_name = \"Boehm_JProteomeRes2014\"\n",
    "yaml_url = (\n",
    "    f\"https://raw.githubusercontent.com/Benchmarking-Initiative/Benchmark-Models-PEtab/\"\n",
    "    f\"master/Benchmark-Models/{model_name}/{model_name}.yaml\"\n",
    ")\n",
    "\n",
    "# Load the PEtab problem from the YAML file\n",
    "# petab_problem = petab.Problem.from_yaml(yaml_url)\n",
    "\n",
    "# Load the PEtab problem from the YAML file as a PEtab v2 problem\n",
    "# (the JAX backend only supports PEtab v2)\n",
    "petab_problem = petab1to2.petab1to2(yaml_url)\n",
    "\n",
    "# Import the PEtab problem as a JAX-compatible AMICI problem\n",
    "pi = PetabImporter(\n",
    "    petab_problem=petab_problem,\n",
    "    module_name=model_name,\n",
    "    compile_=True,\n",
    "    jax=True,\n",
    ")\n",
    "\n",
    "jax_problem = pi.create_simulator(\n",
    "    force_import=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0dce1427a7883f",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "We can now run efficient simulation using [amici.jax.run_simulations]((https://amici.readthedocs.io/en/latest/generated/amici.jax.html#amici.jax.run_simulations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99eccd0607793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amici.jax import run_simulations\n",
    "\n",
    "# Run simulations and compute the log-likelihood\n",
    "llh, results = run_simulations(jax_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b2980-13f0-42e9-b13e-0fce05793910",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415962751301c64a",
   "metadata": {},
   "source": [
    "This simulates the model for all conditions using the nominal parameter values. Simple, right? Now, let’s take a look at the simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b86e45e18fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the simulation condition\n",
    "experiment_condition = \"_petab_experiment_condition___default__\"\n",
    "\n",
    "# # Access the results for the specified condition\n",
    "ic = results[\"dynamic_conditions\"].index(experiment_condition)\n",
    "print(\"llh: \", results[\"llh\"][ic])\n",
    "print(\"state variables: \", results[\"x\"][ic, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b173e013f9210a",
   "metadata": {},
   "source": [
    "Unfortunately, the simulation failed! As seen in the output, the simulation broke down after the initial timepoint, indicated by the `inf` values in the state variables `results['x']` and the `nan` likelihood value. A closer inspection of this variable provides additional clues about what might have gone wrong.\n",
    "\n",
    "The issue stems from using single precision, as indicated by the `float32` dtype of state variables. Single precision is generally a [bad idea](https://docs.kidger.site/diffrax/examples/stiff_ode/) for stiff systems like the Böhm model. Let’s retry the simulation with double precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5ff705a3f7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "# Enable double precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Re-run simulations with double precision\n",
    "llh, results = run_simulations(jax_problem)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d3b40ee3efdf2",
   "metadata": {},
   "source": [
    "Success! The simulation completed successfully, and we can now plot the resulting state trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f06ff19626df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the experiment condition\n",
    "experiment_condition = \"_petab_experiment_condition___default__\"\n",
    "\n",
    "\n",
    "def plot_simulation(results):\n",
    "    \"\"\"\n",
    "    Plot the state trajectories from the simulation results.\n",
    "\n",
    "    Parameters:\n",
    "        results (dict): Simulation results from run_simulations.\n",
    "    \"\"\"\n",
    "    # Extract the simulation results for the specific condition\n",
    "    ic = results[\"dynamic_conditions\"].index(experiment_condition)\n",
    "\n",
    "    # Create a new figure for the state trajectories\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for ix in range(results[\"x\"].shape[2]):\n",
    "        time_points = np.array(results[\"ts\"][ic, :])\n",
    "        state_values = np.array(results[\"x\"][ic, :, ix])\n",
    "        plt.plot(\n",
    "            time_points, state_values, label=jax_problem.model.state_ids[ix]\n",
    "        )\n",
    "\n",
    "    # Add labels, legend, and grid\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"State Values\")\n",
    "    plt.title(experiment_condition)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the simulation results\n",
    "plot_simulation(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa97c33719c2277",
   "metadata": {},
   "source": [
    "`run_simulations` enables users to specify the simulation conditions to be executed. For more complex models, this allows for restricting simulations to a subset of conditions. The Böhm model includes only a single experimental condition, so we have to simulate that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950774a3e989042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llh, results = run_simulations(jax_problem, simulation_experiments=tuple()\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8516a75ce4d12",
   "metadata": {},
   "source": [
    "## Updating Parameters\n",
    "\n",
    "As next step, we will update the parameter values used for simulation. However, if we attempt to directly modify the values in `JAXModel.parameters`, we encounter a `FrozenInstanceError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d278a3d21e709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import FrozenInstanceError\n",
    "\n",
    "import jax\n",
    "\n",
    "# Generate random noise to update the parameters\n",
    "noise = (\n",
    "    jax.random.normal(\n",
    "        key=jax.random.PRNGKey(0), shape=jax_problem.parameters.shape\n",
    "    )\n",
    "    / 10\n",
    ")\n",
    "\n",
    "# Attempt to update the parameters\n",
    "try:\n",
    "    jax_problem.parameters += noise\n",
    "except FrozenInstanceError as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3d595de4a4085",
   "metadata": {},
   "source": [
    "The root cause of this error lies in the fact that, to enable autodiff, direct modifications of attributes are not allowed in [equinox](https://docs.kidger.site/equinox/), which AMICI utilizes under the hood. Consequently, attributes of instances like `JAXModel` or `JAXProblem` cannot be updated directly — this is the price we have to pay for autodiff.\n",
    "\n",
    "However, `JAXProblem` provides a convenient method called [update_parameters](https://amici.readthedocs.io/en/latest/generated/amici.jax.html#amici.jax.JAXProblem.update_parameters). The caveat is that this method creates a new JAXProblem instance instead of modifying the existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47748376059628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters and create a new JAXProblem instance\n",
    "jax_problem = jax_problem.update_parameters(jax_problem.parameters + noise)\n",
    "\n",
    "# Run simulations with the updated parameters\n",
    "llh, results = run_simulations(jax_problem)\n",
    "\n",
    "# Plot the simulation results\n",
    "plot_simulation(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660baf605a4e8339",
   "metadata": {},
   "source": [
    "## Computing Gradients\n",
    "\n",
    "Similar to updating attributes, computing gradients in the JAX ecosystem can feel a bit unconventional if you’re not familiar with the JAX ecosystem. JAX offers [automatic differentiation](https://jax.readthedocs.io/en/latest/automatic-differentiation.html) through the `jax.grad` function. However, to use `jax.grad` with `JAXProblem`, we need to specify which parts of the `JAXProblem` should be treated as static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033d09cc81b7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Attempt to compute the gradient of the run_simulations function\n",
    "    jax.grad(run_simulations, has_aux=True)(jax_problem)\n",
    "except TypeError as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bc07cde00a926",
   "metadata": {},
   "source": [
    "Fortunately, `equinox` simplifies this process by offering [filter_grad](https://docs.kidger.site/equinox/api/transformations/#equinox.filter_grad), which enables autodiff functionality that is compatible with `JAXProblem` and, in theory, also with `JAXModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6704182200e6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "\n",
    "# Compute the gradient using equinox's filter_grad, preserving auxiliary outputs\n",
    "grad, _ = eqx.filter_grad(run_simulations, has_aux=True)(jax_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c3ec94cb5d086",
   "metadata": {},
   "source": [
    "Functions transformed by `filter_grad` return gradients that share the same structure as the first argument (unless specified otherwise). This allows us to access the gradient with respect to the parameters attribute directly `via grad.parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c1581d7173d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b835fecc5a022",
   "metadata": {},
   "source": [
    "Attributes for which derivatives cannot be computed (typically anything that is not a [jax.numpy.array](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)) are automatically set to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c17f7459d0151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7cc3db510c826",
   "metadata": {},
   "source": [
    "Observant readers may notice that the gradient above appears to include numeric values for derivatives with respect to some measurements. However, `simulation_conditions` internally disables gradient computations using `jax.lax.stop_gradient`, resulting in these values being zeroed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3badd4402cf6b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad._my[ic, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb04393a1463d",
   "metadata": {},
   "source": [
    "However, we can compute derivatives with respect to data elements using `JAXModel.simulate_condition`. In the example below, we differentiate the observables `y` (specified by passing `y` to the `ret` argument) with respect to the timepoints at which the model outputs are computed after the solving the differential equation. While this might not be particularly practical, it serves as an nice illustration of the power of automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91aff44b93157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffrax\n",
    "import jax.numpy as jnp\n",
    "import optimistix\n",
    "from amici.jax import ReturnValue\n",
    "\n",
    "# Define the simulation condition\n",
    "experiment_condition = \"_petab_experiment_condition___default__\"\n",
    "ic = 0\n",
    "\n",
    "# Load condition-specific data\n",
    "ts_dyn = jax_problem._ts_dyn[ic, :]\n",
    "ts_posteq = jax_problem._ts_posteq[ic, :]\n",
    "my = jax_problem._my[ic, :]\n",
    "iys = jax_problem._iys[ic, :]\n",
    "iy_trafos = jax_problem._iy_trafos[ic, :]\n",
    "ops = jax_problem._op_numeric[ic, :]\n",
    "nps = jax_problem._np_numeric[ic, :]\n",
    "\n",
    "# Load parameters for the specified condition\n",
    "p = jax_problem.load_model_parameters(jax_problem._petab_problem.experiments[0], is_preeq=False)\n",
    "\n",
    "\n",
    "# Define a function to compute the gradient with respect to dynamic timepoints\n",
    "@eqx.filter_jacfwd\n",
    "def grad_ts_dyn(tt):\n",
    "    return jax_problem.model.simulate_condition(\n",
    "        p=p,\n",
    "        ts_dyn=tt,\n",
    "        ts_posteq=ts_posteq,\n",
    "        my=jnp.array(my),\n",
    "        iys=jnp.array(iys),\n",
    "        iy_trafos=jnp.array(iy_trafos),\n",
    "        ops=jnp.array(ops),\n",
    "        nps=jnp.array(nps),\n",
    "        solver=diffrax.Kvaerno5(),\n",
    "        controller=diffrax.PIDController(atol=1e-8, rtol=1e-8),\n",
    "        root_finder=optimistix.Newton(atol=1e-8, rtol=1e-8),\n",
    "        steady_state_event=diffrax.steady_state_event(),\n",
    "        max_steps=2**10,\n",
    "        adjoint=diffrax.DirectAdjoint(),\n",
    "        ret=ReturnValue.y,  # Return observables\n",
    "    )[0]\n",
    "\n",
    "\n",
    "# Compute the gradient with respect to `ts_dyn`\n",
    "g = grad_ts_dyn(ts_dyn)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca88c8900584ce",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99c046d7d4e225",
   "metadata": {},
   "source": [
    "This setup makes it pretty straightforward to train models using [equinox](https://docs.kidger.site/equinox/) and [optax](https://optax.readthedocs.io/en/latest/) frameworks. Below we provide barebones implementation that runs training for 5 steps using Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf05f7866bd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optax import adam\n",
    "\n",
    "# define loss function\n",
    "loss = eqx.filter_value_and_grad(run_simulations, has_aux=True)\n",
    "\n",
    "# initialise adam\n",
    "optim = adam(0.01)\n",
    "# eqx.partition is necessary here to only initialize the optimizer for array variables\n",
    "param, static = eqx.partition(jax_problem, eqx.is_array)\n",
    "opt_state = optim.init(param)\n",
    "\n",
    "\n",
    "# define update function\n",
    "@eqx.filter_jit\n",
    "def make_step(problem, opt_state):\n",
    "    current_loss, grads = loss(problem)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(problem, updates)\n",
    "    return current_loss, model, opt_state\n",
    "\n",
    "\n",
    "# run 5 optimisation steps\n",
    "for step in range(5):\n",
    "    current_loss, jax_problem, opt_state = make_step(jax_problem, opt_state)\n",
    "    current_loss = current_loss[0].item()\n",
    "    print(f\"step={step}, loss={current_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f870da7754e139c",
   "metadata": {},
   "source": [
    "## Compilation & Profiling\n",
    "\n",
    "To maximize performance with JAX, code should be just-in-time (JIT) compiled. This can be achieved using the `jax.jit` or `equinox.filter_jit` decorators. While JIT compilation introduces some overhead during the first function call, it significantly improves performance for subsequent calls. To demonstrate this, we will first clear the JIT cache and then profile the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebdc110ea7457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# Clear JAX caches to ensure a fresh start\n",
    "jax.clear_caches()\n",
    "\n",
    "# Define a JIT-compiled gradient function with auxiliary outputs\n",
    "gradfun = eqx.filter_jit(eqx.filter_grad(run_simulations, has_aux=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1242075f7e0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the time taken for the first function call (including compilation)\n",
    "start = time()\n",
    "run_simulations(jax_problem)\n",
    "print(f\"Function compilation time: {time() - start:.2f} seconds\")\n",
    "\n",
    "# Measure the time taken for the gradient computation (including compilation)\n",
    "start = time()\n",
    "gradfun(jax_problem)\n",
    "print(f\"Gradient compilation time: {time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27181f367ccb1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "run_simulations(\n",
    "    jax_problem,\n",
    "    controller=diffrax.PIDController(\n",
    "        rtol=1e-8,  # same as amici default\n",
    "        atol=1e-16,  # same as amici default\n",
    "        pcoeff=0.4,  # recommended value for stiff systems\n",
    "        icoeff=0.3,  # recommended value for stiff systems\n",
    "        dcoeff=0.0,  # recommended value for stiff systems\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d3a6162a3ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "gradfun(\n",
    "    jax_problem,\n",
    "    controller=diffrax.PIDController(\n",
    "        rtol=1e-8,  # same as amici default\n",
    "        atol=1e-16,  # same as amici default\n",
    "        pcoeff=0.4,  # recommended value for stiff systems\n",
    "        icoeff=0.3,  # recommended value for stiff systems\n",
    "        dcoeff=0.0,  # recommended value for stiff systems\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733a450635a749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amici.sim.sundials import SensitivityMethod, SensitivityOrder\n",
    "from amici.sim.sundials.petab.v1 import simulate_petab\n",
    "\n",
    "# Import the PEtab problem as a standard AMICI model\n",
    "pi = PetabImporter(\n",
    "    petab_problem=petab_problem,\n",
    "    module_name=model_name,\n",
    "    compile_=True,\n",
    "    jax=False,\n",
    ")\n",
    "\n",
    "amici_model = pi.create_simulator(\n",
    "    force_import=True,\n",
    ")\n",
    "\n",
    "# Configure the solver with appropriate tolerances\n",
    "amici_model.solver.set_absolute_tolerance(1e-8)\n",
    "amici_model.solver.set_relative_tolerance(1e-16)\n",
    "\n",
    "# Prepare the parameters for the simulation\n",
    "problem_parameters = dict(\n",
    "    zip(jax_problem.parameter_ids, jax_problem.parameters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ed7c60b2cf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile simulation only\n",
    "amici_model.solver.set_sensitivity_order(SensitivityOrder.none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbc67bc09b67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "amici_model.simulate(petab_problem.get_x_nominal_dict())\n",
    "# simulate_petab(\n",
    "#     petab_problem,\n",
    "#     amici_model=amici_model,\n",
    "#     solver=solver,\n",
    "#     problem_parameters=problem_parameters,\n",
    "#     scaled_parameters=True,\n",
    "#     scaled_gradients=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c06c5893a9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile gradient computation using forward sensitivity analysis\n",
    "amici_model.solver.set_sensitivity_order(SensitivityOrder.first)\n",
    "amici_model.solver.set_sensitivity_method(SensitivityMethod.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367a19bcea98597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "amici_model.simulate(petab_problem.get_x_nominal_dict())\n",
    "# simulate_petab(\n",
    "#     petab_problem,\n",
    "#     amici_model=amici_model,\n",
    "#     solver=solver,\n",
    "#     problem_parameters=problem_parameters,\n",
    "#     scaled_parameters=True,\n",
    "#     scaled_gradients=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e8eda806c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile gradient computation using adjoint sensitivity analysis\n",
    "amici_model.solver.set_sensitivity_order(SensitivityOrder.first)\n",
    "amici_model.solver.set_sensitivity_method(SensitivityMethod.adjoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ab1acb3ba818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# amici_model.simulate(petab_problem.get_x_nominal_dict())\n",
    "# Results in an error regarding the FIM\n",
    "\n",
    "# simulate_petab(\n",
    "#     petab_problem,\n",
    "#     amici_model=amici_model,\n",
    "#     solver=solver,\n",
    "#     problem_parameters=problem_parameters,\n",
    "#     scaled_parameters=True,\n",
    "#     scaled_gradients=True,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
